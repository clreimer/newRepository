{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up\n",
    "\n",
    "## Import Datalogue libraries\n",
    "\n",
    "Note, you'll need to have downloaded and installed the Datalogue SDK before this step will work.\n",
    "\n",
    "Right now, to do so you will need to get access through Artifactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.31.1'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Datalogue libraries \n",
    "from datalogue import *\n",
    "from datalogue.version import __version__\n",
    "from datalogue.models.ontology import *\n",
    "from datalogue.models.datastore_collection import *\n",
    "from datalogue.models.datastore import *\n",
    "from datalogue.models.datastore import GCSDatastoreDef \n",
    "from datalogue.models.credentials import *\n",
    "from datalogue.models.stream import *\n",
    "from datalogue.models.transformations import *\n",
    "from datalogue.models.transformations.structure import *\n",
    "from datalogue.dtl import Dtl, DtlCredentials\n",
    "from datalogue.models.training import DataRef\n",
    "\n",
    "# Import Datalogue Bag of Tricks\n",
    "from DTLBagOTricks import DTL as DTLHelper\n",
    "\n",
    "\n",
    "# Import other useful libraries\n",
    "from datetime import datetime, timedelta\n",
    "from os import environ\n",
    "import pandas\n",
    "from IPython.display import Image\n",
    "\n",
    "# Checks the version of the SDK is correct\n",
    "# The expected version is 0.28.3\n",
    "# If the SDK is not installed, run `! pip install datalogue` and restart the Jupyter Notebook kernel\n",
    "# If the wrong versions is installed, run `! pip install datalogue --upgrade` and restart the Jupyter Notebook kernel\n",
    "__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datalogue v0.31.1\n",
      "Logged in 'https://internal.dtl.systems/api' with 'chrisr@datalogue.io' account.\n"
     ]
    }
   ],
   "source": [
    "# Set host, username and password variables\n",
    "datalogue_host = \"https://internal.dtl.systems\"  # for connecting to internal (note)\n",
    "\n",
    "#email = environ.get(\"DTL_EMAIL\")\n",
    "email = \"chrisr@datalogue.io\"\n",
    "#password = environ.get(\"DTL_PASSWORD\")\n",
    "password = \"StreudelSauce1!\"\n",
    "\n",
    "# Log in to Datalogue\n",
    "BOT = DTLHelper(datalogue_host, email, password)\n",
    "dtl = BOT.dtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce9ced64-dbc6-468e-b86a-e3941c025e37\n",
      "ffcc1ec1-89df-4df8-bac4-0db036f9977f\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy the model before the tidy up to give it some time to be ready:\n",
    "\n",
    "from datalogue.models.training import *\n",
    "import uuid\n",
    "\n",
    "OntologyId = 'ce9ced64-dbc6-468e-b86a-e3941c025e37'\n",
    "trainingId = dtl.training.get_trainings(uuid.UUID(str(OntologyId)))[0].id\n",
    "\n",
    "print(OntologyId)\n",
    "print(trainingId)\n",
    "\n",
    "dtl.training.deploy(trainingId, OntologyId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's clean up the assets this workbook creates from previous runs\n",
    "\n",
    "# Warning! this will clean all your datastores and data collections and credentials\n",
    "\n",
    "#BOT.server_summary()\n",
    "\n",
    "# Clear Datastores and Datastore Collections\n",
    "for store in dtl.datastore.list():\n",
    "#    print(store.name, ',', store.name[:5])\n",
    "    if (store.name == 'dtl-PG-cpg'):\n",
    "        targetDS = store.id\n",
    "        \n",
    "    if (store.name[:5] == 'demo-'):\n",
    "        dtl.datastore.delete(store.id)\n",
    "\n",
    "for store in dtl.datastore_collection.list():\n",
    "#    print(store.name)\n",
    "    if (store.name[:5] == 'demo-'):    \n",
    "        dtl.datastore_collection.delete(store.id)\n",
    "\n",
    "# Clear data pipelines\n",
    "for StreamCollection in dtl.stream_collection.list():\n",
    "#    print(StreamCollection, '\\n')\n",
    "    if (StreamCollection.name[:5] == 'demo-'):\n",
    "        dtl.stream_collection.delete(StreamCollection.id)\n",
    "    if (StreamCollection.name == ''):\n",
    "        dtl.stream_collection.delete(StreamCollection.id)\n",
    "\n",
    "## Clear ontologies\n",
    "for Ontology in dtl.ontology.list():\n",
    "#     dtl.ontology.delete(ontology.id)\n",
    "    if (Ontology.name[:12] == 'CPG Ontology'):\n",
    "        OntologyID = Ontology.id\n",
    "\n",
    "#BOT.server_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the PostgreSQL database...\n",
      "PostgreSQL database version:\n",
      "('PostgreSQL 11.5 on x86_64-pc-linux-gnu, compiled by gcc (Debian 7.3.0-5) 7.3.0, 64-bit',)\n",
      "relation \"cpg\" does not exist\n",
      "\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# truncate the postgres table before writing data\n",
    "\n",
    "import psycopg2\n",
    "#from config import Config\n",
    " \n",
    "def connect():\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # read connection parameters\n",
    "        #params = config()\n",
    " \n",
    "        # connect to the PostgreSQL server\n",
    "        print('Connecting to the PostgreSQL database...')\n",
    "        #conn = psycopg2.connect(**params)\n",
    "        conn = psycopg2.connect(host=\"34.73.161.131\",database=\"demo\", user=\"postgres\", password=\"devout-north-solitude\")\n",
    "\n",
    "      \n",
    "        # create a cursor\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "   # execute a statement\n",
    "        print('PostgreSQL database version:')\n",
    "        cur.execute('SELECT version()')\n",
    " \n",
    "        # display the PostgreSQL database server version\n",
    "        db_version = cur.fetchone()\n",
    "        print(db_version)\n",
    "        \n",
    "        # truncate the table\n",
    "        cur.execute('TRUNCATE TABLE cpg')\n",
    "        \n",
    "       # close the communication with the PostgreSQL\n",
    "        cur.close()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            print('Database connection closed.')\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Source Files from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n",
      "<class 'dict'>\n",
      "[{'store_name': 'demo-CPG/raw_data/1_retail_UK_IRE_NED_NOR.csv', 'URL': 'https://datalogue-demo.s3.amazonaws.com/CPG/raw_data/1_retail_UK_IRE_NED_NOR.csv'}, {'store_name': 'demo-CPG/raw_data/2_retail_BAH_BRA_RSA.csv', 'URL': 'https://datalogue-demo.s3.amazonaws.com/CPG/raw_data/2_retail_BAH_BRA_RSA.csv'}, {'store_name': 'demo-CPG/raw_data/3_retail_FR_GER.json', 'URL': 'https://datalogue-demo.s3.amazonaws.com/CPG/raw_data/3_retail_FR_GER.json'}]\n"
     ]
    }
   ],
   "source": [
    "from boto.s3.connection import S3Connection\n",
    "\n",
    "conn = S3Connection('AKIAIXM6CXHGHC62R7GA','Gcb34qctsvPoQJGGDrXzmwMbyaCZOg6zY1RFOVQO')\n",
    "bucket = conn.get_bucket('datalogue-demo')\n",
    "\n",
    "keys = [\"store_name\", \"URL\"]\n",
    "cpg_data = []\n",
    "\n",
    "for key in bucket.list():\n",
    "    if 'retail' in key.name:\n",
    "        url=\"https://datalogue-demo.s3.amazonaws.com/\" + key.name\n",
    "        values = [\"demo-\"+key.name, url]\n",
    "        cpg_data.append(dict(zip(keys, values)))\n",
    "            \n",
    "print(type(cpg_data))\n",
    "print(len(cpg_data))\n",
    "print(type(cpg_data[0]))\n",
    "print(cpg_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV Sources to connect to:\n",
      "-------------------------\n",
      "➜ demo-CPG/raw_data/1_retail_UK_IRE_NED_NOR.csv\n",
      "➜ demo-CPG/raw_data/2_retail_BAH_BRA_RSA.csv\n",
      "➜ demo-CPG/raw_data/3_retail_FR_GER.json\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nCSV Sources to connect to:\\n\" \"-------------------------\")\n",
    "for data_store in cpg_data:\n",
    "    print(\"➜ \" + data_store[\"store_name\"])\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create datastore connections for each file in S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo-CPG/raw_data/1_retail_UK_IRE_NED_NOR.csv --- .csv\n",
      "demo-CPG/raw_data/2_retail_BAH_BRA_RSA.csv --- .csv\n",
      "demo-CPG/raw_data/3_retail_FR_GER.json --- json\n"
     ]
    }
   ],
   "source": [
    "current_stores = []\n",
    "\n",
    "for data_store in cpg_data:\n",
    "    print(data_store[\"store_name\"], \"---\", data_store[\"store_name\"][-4:])\n",
    "    if (data_store[\"store_name\"][-4:] == '.csv'):\n",
    "        data_store[\"datastore_object\"] = dtl.datastore.create(\n",
    "            Datastore(\n",
    "                data_store[\"store_name\"],\n",
    "                HttpDatastoreDef(data_store[\"URL\"], FileFormat.Csv),))\n",
    "    if (data_store[\"store_name\"][-4:] == '.xml'):\n",
    "        data_store[\"datastore_object\"] = dtl.datastore.create(\n",
    "            Datastore(\n",
    "                data_store[\"store_name\"],\n",
    "                HttpDatastoreDef(data_store[\"URL\"], FileFormat.Xml),))\n",
    "    if (data_store[\"store_name\"][-4:] == 'json'):\n",
    "        data_store[\"datastore_object\"] = dtl.datastore.create(\n",
    "            Datastore(\n",
    "                data_store[\"store_name\"],\n",
    "                HttpDatastoreDef(data_store[\"URL\"], FileFormat.Json),))\n",
    "\n",
    "    current_stores.append(data_store[\"datastore_object\"])\n",
    "\n",
    "#print(type(current_stores))\n",
    "#print(type(current_stores[0]))\n",
    "#print(current_stores[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###           3b. Create datastore for RDBMS target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host: 34.74.11.127 (use jdbc:postgresql://34.74.11.127:5432/demo for creating target store)\n",
    "# user: postgres\n",
    "# pw: L8am0pO5zjJrFm2O\n",
    "\n",
    "# bug in SDK for v<1.0; to be updated here but created in GUI for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collecting data stores into a collection\n",
    "\n",
    "This is just used for organization, and uses the command `dtl.datastore_collection.create`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpg_collection = DatastoreCollection(\n",
    "  name =\"demo-CPG Collection\",\n",
    "  storeIds = [Datastore[\"datastore_object\"].id for Datastore in cpg_data],\n",
    "  description = \"Manufacturing data of various formats\"\n",
    ")\n",
    "cpg_collection2 = dtl.datastore_collection.create(cpg_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce9ced64-dbc6-468e-b86a-e3941c025e37\n",
      "ffcc1ec1-89df-4df8-bac4-0db036f9977f\n"
     ]
    }
   ],
   "source": [
    "from datalogue.models.training import *\n",
    "import uuid\n",
    "\n",
    "#OntologyId = 'ce9ced64-dbc6-468e-b86a-e3941c025e37'\n",
    "#trainingId = dtl.training.get_trainings(uuid.UUID(str(OntologyID)))[0].id\n",
    "\n",
    "print(OntologyId)\n",
    "print(trainingId)\n",
    "\n",
    "#dtl.training.deploy(trainingId, OntologyId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datastore(id: 2e2db0a4-b4a6-43b1-b9ae-b4c006956731, name: 'dtl-PG-cpg', alias: None, credential_id: 970d851a-e74d-4edf-b604-af18ffdaf009, definition: <datalogue.models.datastore.JdbcDatastoreDef object at 0x000001DE59E03320>, samples: None, schema_paths: [], schema_labels: [], schema_nodes: None)\n"
     ]
    }
   ],
   "source": [
    "#set target of stream:\n",
    "\n",
    "my_output_store = dtl.datastore.get(targetDS)\n",
    "print(my_output_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target output schema transformation using 'structure'\n",
    "\n",
    "std_schema = Structure([\n",
    "        ClassNodeDescription(\n",
    "            path = [\"InvoiceNo2\"],\n",
    "            tag = \"InvoiceNo\",\n",
    "            pick_strategy = PickStrategy.HighScore,\n",
    "            data_type = DataType.String\n",
    "        ),\n",
    "        ClassNodeDescription(\n",
    "            path = [\"UnitPrice2\"],\n",
    "            tag = \"UnitPrice\",\n",
    "            pick_strategy = PickStrategy.HighScore,\n",
    "            data_type = DataType.String\n",
    "        ),\n",
    "        ClassNodeDescription(\n",
    "            path = [\"SKU2\"],\n",
    "            tag = \"SKU\",\n",
    "            pick_strategy = PickStrategy.HighScore,\n",
    "            data_type = DataType.String\n",
    "        ),\n",
    "        ClassNodeDescription(\n",
    "            path = [\"Description2\"],\n",
    "            tag = \"Description\",\n",
    "            pick_strategy = PickStrategy.HighScore,\n",
    "            data_type = DataType.String\n",
    "        ),\n",
    "        ClassNodeDescription(\n",
    "            path = [\"Quantity2\"],\n",
    "            tag = \"Quantity\",\n",
    "            pick_strategy = PickStrategy.HighScore,\n",
    "            data_type = DataType.String\n",
    "        ),\n",
    "        ClassNodeDescription(\n",
    "            path = [\"CustomerID2\"],\n",
    "            tag = \"CustomerID\",\n",
    "            pick_strategy = PickStrategy.HighScore,\n",
    "            data_type = DataType.String\n",
    "        ),\n",
    "        ClassNodeDescription(\n",
    "            path = [\"InvoiceDate2\"],\n",
    "            tag = \"InvoiceDate\",\n",
    "            pick_strategy = PickStrategy.HighScore,\n",
    "            data_type = DataType.String\n",
    "        ),\n",
    "        ClassNodeDescription(\n",
    "            path = [\"Country2\"],\n",
    "            tag = \"Country\",\n",
    "            pick_strategy = PickStrategy.HighScore,\n",
    "            data_type = DataType.String\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffcc1ec1-89df-4df8-bac4-0db036f9977f\n"
     ]
    }
   ],
   "source": [
    "from datalogue.models.training import *\n",
    "import uuid\n",
    "\n",
    "modelUuid = dtl.training.get_trainings(uuid.UUID(str(OntologyID)))[0].id\n",
    "print(modelUuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classify transformation\n",
    "\n",
    "#tx_definition = Definition(    # (List[Transformation], pipelines: List['Definition'], target_datastore )\n",
    "#            [\n",
    "#                Classify(training_id = modelUuid, use_context=True, include_classes=False, include_scores=False),\n",
    "#                std_schema\n",
    "#            ], # List of transformations\n",
    "#        [], # pipelines list\n",
    "#            my_output_store, # target_datastore\n",
    "#        )\n",
    "\n",
    "#print(type(tx_definition))\n",
    "#print(tx_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "ffcc1ec1-89df-4df8-bac4-0db036f9977f\n"
     ]
    }
   ],
   "source": [
    "x = dtl.training.get_trainings('ce9ced64-dbc6-468e-b86a-e3941c025e37')\n",
    "print(type(x))\n",
    "print(x[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datalogue.models.stream.Definition'>\n",
      "Pipeline(type: [Classify(classifier: Classifier(default_class: None classification_methods: \n",
      "[MLMethod(\n",
      " model_id: ffcc1ec1-89df-4df8-bac4-0db036f9977f, threshold: None\n",
      ")]), fields_to_target: None, add_class_fields: False, add_score_fields: False), Structure(structure: [ClassNodeDescription(path: ['InvoiceNo2'], tag: InvoiceNo, strategy: PickStrategy.HighScore, dataType: DataType.String), ClassNodeDescription(path: ['UnitPrice2'], tag: UnitPrice, strategy: PickStrategy.HighScore, dataType: DataType.String), ClassNodeDescription(path: ['SKU2'], tag: SKU, strategy: PickStrategy.HighScore, dataType: DataType.String), ClassNodeDescription(path: ['Description2'], tag: Description, strategy: PickStrategy.HighScore, dataType: DataType.String), ClassNodeDescription(path: ['Quantity2'], tag: Quantity, strategy: PickStrategy.HighScore, dataType: DataType.String), ClassNodeDescription(path: ['CustomerID2'], tag: CustomerID, strategy: PickStrategy.HighScore, dataType: DataType.String), ClassNodeDescription(path: ['InvoiceDate2'], tag: InvoiceDate, strategy: PickStrategy.HighScore, dataType: DataType.String), ClassNodeDescription(path: ['Country2'], tag: Country, strategy: PickStrategy.HighScore, dataType: DataType.String)])], pipelines: [], target: <datalogue.models.datastore.JdbcDatastoreDef object at 0x000001DE59E03320>)\n"
     ]
    }
   ],
   "source": [
    "# Define classify transformation\n",
    "from datalogue.models.transformations.classify import Classifier, MLMethod\n",
    "tx_definition = Definition(    # (List[Transformation], pipelines: List['Definition'], target_datastore )\n",
    "            [\n",
    "                Classify(Classifier([MLMethod('ffcc1ec1-89df-4df8-bac4-0db036f9977f')])),\n",
    "                std_schema\n",
    "            ], # List of transformations\n",
    "        [], # pipelines list\n",
    "            my_output_store, # target_datastore\n",
    "        )\n",
    "\n",
    "print(type(tx_definition))\n",
    "print(tx_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  streams in collection\n"
     ]
    }
   ],
   "source": [
    "# Define n stream(s), where n is number of datastore connections created from S3 bucket scan\n",
    "n = len(current_stores)\n",
    "i = 1\n",
    "\n",
    "list_of_streams = []\n",
    "for i in range(n):\n",
    "    stream = Stream(current_stores[i], [tx_definition])\n",
    "    i += 1\n",
    "    list_of_streams.append(stream)\n",
    "\n",
    "print(len(list_of_streams), ' streams in collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the streams in a collection\n",
    "\n",
    "stream_collection = dtl.stream_collection.create(\n",
    "    list_of_streams,\n",
    "    'demo-CPGPipeline'\n",
    ")\n",
    "\n",
    "#print(type(stream_collection))\n",
    "#print(stream_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Job(id: UUID('4215f519-16a3-4253-8d24-86a6522011b8'), stream_id: UUID('f9a32cc6-3b72-437e-badd-4fcce91ea5f8'), stream_collection_id: UUID('bc4b73b9-6d85-43d2-835e-1c49f3169073'), status: Scheduled, run_at: datetime.datetime(2019, 11, 21, 22, 23, 54, tzinfo=tzutc()), created_by: UUID('5b333964-8fab-4ab0-9052-25f69fcb8689'), remaining_time_millis: 9223372036854775807, percent_progress: 0, errors: None, ended_at: None,\n",
       " Job(id: UUID('0415a783-355c-42d3-b0ee-56e620faab3d'), stream_id: UUID('3a10fbe9-ccb2-440f-967c-1ca3d822e5fa'), stream_collection_id: UUID('bc4b73b9-6d85-43d2-835e-1c49f3169073'), status: Scheduled, run_at: datetime.datetime(2019, 11, 21, 22, 23, 54, tzinfo=tzutc()), created_by: UUID('5b333964-8fab-4ab0-9052-25f69fcb8689'), remaining_time_millis: 9223372036854775807, percent_progress: 0, errors: None, ended_at: None,\n",
       " Job(id: UUID('ec09de69-80b3-4460-b98e-659c03b554a0'), stream_id: UUID('4a731ac8-7119-44d4-a4cc-9a335487766f'), stream_collection_id: UUID('bc4b73b9-6d85-43d2-835e-1c49f3169073'), status: Scheduled, run_at: datetime.datetime(2019, 11, 21, 22, 23, 54, tzinfo=tzutc()), created_by: UUID('5b333964-8fab-4ab0-9052-25f69fcb8689'), remaining_time_millis: 9223372036854775807, percent_progress: 0, errors: None, ended_at: None]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Collection\n",
    "\n",
    "dtl.stream_collection.run(stream_collection.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
